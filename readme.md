1st task.
    Create a crawler, 
    download more or equal to 100 pages' texts, 
    store them to archive,
    create an index.txt file that contains pages' IDs and their full links

We use Scrapy framework to do this task. It provides a functionality for web-crawlers.
To run spider:
**cd crawler**
**scrapy crawl quotes**